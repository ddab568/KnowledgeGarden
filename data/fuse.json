{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Knowledge Garden","n":0.707},"1":{"v":"\n#### My Second Brain\n\n---\n\nA collection of my notes around all things software development.\n\n> This website is generated by a Dendron template. For more information, see the [template README file](https://github.com/dendronhq/template.publish.github/).\n","n":0.186}}},{"i":2,"$":{"0":{"v":"Web","n":1}}},{"i":3,"$":{"0":{"v":"Search Engine Optimization (SEO)","n":0.5},"1":{"v":"\nGeneral guide for achieving good SEO for website:\n\nThree Pillars of optimization according to NextJS:\n1. **Technical** - optimization/performance of the website\n2. **Creation** - ensuring optimal content creation to target specific keywords\n3. **Popularity** - Using backlinks (third party sites linked back to your own) so that search engines know you are a trusted source. Having an online presence. \n\n\n**Web crawlers** are a type of bot that mimic end users and navigate links found on websites in order to perform indexing for search engines. They identify themselves through custom [user-agents](https://developers.google.com/search/docs/advanced/crawling/overview-google-crawlers). \n\n\n#### Notes on improving SEO\n- Build a sitemap\n- Use SSR or SSGs - see [[web.frontend.rendering-strategies]]. SPAs usually don't use url paths and loading time is not ideal for a crawler.\n- Use metadata such as `<title>` and `<meta name=\"description\" content=\"A good description here\">` on every page\n  -  robot tags are directives search engines will respect e.g. `<meta name=\"robots\" content=\"noindex,nofollow\">` . These meta tags can be produced on demand as opposed to robots.txt\n  - canonical tags to prevent the detection of duplicate content\n- Ensure good performance e.g. response time\n- Use alt for images\n- Use meaningful status codes\n   - 200 in general\n   - 301 for redirects to indicate permanently moved. Next JS Uses 308 redirects instead to preserve request methods used. 302 and 301s change the request method of the redirect to a GET\n- Use semantic HTML - the correct and most relevant tags\n- Use a good URL structure e.g. semantic urls /learn/nextjs-app-course vs /courses/1\n\nIn November 2020, Google added three new metrics to their page experience signals called Core Web Vitals:\n\n- Largest Contentful Paint (LCP) - The amount of time it takes to render the largest content element within the viewable area of the screen e.g. featured image\n- First Input Delay (FID) - the time between the first users input and when the page responds (responsiveness)\n- Cumulative Layout Shift (CLS) - visual stability of a page - checking for \"janky-ness\" where the screen shifts\n\n### Largest Contentful Paint (LCP)\n\nCommon causes of poor LCP:\n\n- **Slow server response times **- The first thing to look at is Time to First Byte (TFFB) and see if that can be improved.\n\n- **Render blocking JS and CSS** - style sheets and javascript imports will block the HTML parser. Defer any non-critical JavaScript & CSS. CSS and JS files can be minified.\n\n- Slow resource load times - optimize and compress images\n\n- Client side rendering - If you're building a site that is mostly rendered on the client, you should be wary of the effect it can have on LCP if a large JS bundle is used.\n\n### References\n\n1. https://nextjs.org/learn/seo/crawling-and-indexing/status-codes\n2. https://nextjs.org/learn/seo/rendering-and-ranking/rendering-strategies","n":0.048}}},{"i":4,"$":{"0":{"v":"Project Checklist","n":0.707},"1":{"v":"\nCreating a new project considerations, particularly for a javascript full-stack application.\n","n":0.302}}},{"i":5,"$":{"0":{"v":"pixiJS","n":1},"1":{"v":"\nNotes on PIXI JS\n\n- A Container is the most basic class you have. It has no graphical representation but it's used to group objects as a unit\n- A Sprite is the simplest way to show a bitmap on your screen.\n- Sprites can show graphics and Containers are used to group sprites to move, rotate and scale them as a whole.\n- You can add containers to other containers.\n- The root of everything is the Stage. The stage is the regular container the application class creates and feeds to the Renderer to render\n- Anything shown on the screen must inherit from the abstract class `DisplayObject` e.g. Sprite, Container all inherit from this class\n- A DisplayObject can only have one parent\n- A Particle Container is a special kind of container designed to go fast.\n- Graphics allows you to make primitive drawings like rectangles, circles and lines. It's useful for simple graphics without a bitmap file.\n- Ticker is an object that will call a function every frame before rendering\n","n":0.078}}},{"i":6,"$":{"0":{"v":"JavaScript","n":1}}},{"i":7,"$":{"0":{"v":"Commonjs","n":1},"1":{"v":"\nCommonJS\n","n":1}}},{"i":8,"$":{"0":{"v":"Frontend","n":1}}},{"i":9,"$":{"0":{"v":"Web Components","n":0.707},"1":{"v":"\nWeb components allow you to create custom reusable and encapuslated HTML tags to be used in web pages and apps. They are capable of isolating CSS and JavaScript similar to `<iframe>` - known as a **shadown DOM**. These are framework agnostic and are supported by most modern browsers.\n\nExample:\n\n```JavaScript\n// CREATE\nclass MyButton extends HTMLElement {...}\nwindow.customElements.define('my-button', MyButton);\n\n// USE\n<my-button></my-button>\n```\n\n### Web Components vs React\n\nAccording to [React documentation](https://reactjs.org/docs/web-components.html#:~:text=React%20and%20Web%20Components%20are,The%20two%20goals%20are%20complementary.), Web Components and React solve different problems. React is a declarative JS library to address state management and web app \"reactivity\". Web components exist to address reusability and encapsulation of HTML elements. React components can only be used in React, where as web components can be used anywhere e.g. across different libraries - Vue, React, Angular etc.\n","n":0.092}}},{"i":10,"$":{"0":{"v":"Rendering Design Patterns / Strategies","n":0.447},"1":{"v":"\n## CSR (Client-Side Rendering) / SPA (Single Page Application)\n\n> the data is fetched after every single render\n\nMost common way of using frameworks such as React, vue, ember etc.\n\nClient-side rendering means that a website’s JavaScript is rendered in your browser, rather than on the website’s server.\n\n- The server renders a blank page with a script tag pointing to the apps bundle\n- blank page is sent to the clients browser which makes the relevant api calls and renders the contents of the page\n\nstrengths:\n\n- renders fast on the server\n\nweaknesses:\n\n- no initial render - customer/client has to render so if the app is big it could be a problem\n\n- empty body - no content to crawl for a search engine - ** SEO is the biggest weak point **\n\n## SSR (Server Side Rendering)\n\n> the data is fetched before every single render\n\nWhen the user makes a request to the webpage, the server prepares the html page and then sends it back.\n\n## SSG (Static-Site Generating)\n\n> the data is fetched once at build time\n\nPage is also generated on the server however it is rendered at **build time.**\n\n## ISR (Incremental Static Regeneration)\n\n> the data is fetched once on build time and will be fetched again after a certain cooldown and served on the second visit.\n\nAllows you to create or update static pages after you've built your site. This gives you the benefit of using static generation without needing to rebuild the entire site.\n\n## When to use what?\n\n- Is your content dynamic or relatively static?\n\n- is SEO important?\n\nHere are some examples from [this article](https://medium.com/codex/web-design-patterns-ssr-ssg-and-spa-fadad7673dfe#:~:text=SSR%20vs%20SSG,I%20use%20SSR%20then%3F%22.)\n\n> For a documentation page or a website that doesn't need to fetch dynamic data, but SEO is important, you could use SSG.\n> For a blog, SEO is important and the content is dynamic. In this case, SSR could be a good candidate.\n> For a CRM (Customer Relationship Manager), SEO is not a concern and the content is dynamic therefore an SPA/CSR would be a good choice.\n\nMany websites will use a combination. These days for anything that is not an SPA, it's often recommended to use SSG practically all of the time unless there is a good reason not to e.g. the website is massive with dynamic content that cannot be pre-rendered as it changes frequently. [Next JS recommends SSG ](https://nextjs.org/docs/basic-features/pages#pre-rendering) for many use cases. The alternative is to use ISR for a website with mostly static content but for a web page with thousands of pages in order to prevent a complete rebuild every time.\n\n> \"We recommend using Static Generation over Server-side Rendering for performance reasons. Statically generated pages can be cached by CDN with no extra configuration to boost performance. However, in some cases, Server-side Rendering might be the only option.\"\n\nSome important considerations:\n\n1. Data Integrity - how fresh do we want the data? CSR and SSR fetch on every render.\n\n2. SEO - SSR, SSG and ISR are SEO friendly given crawlers don't have to wait for a page to load to get relevant data\n3. Performance - SSG and ISR load instantly (for first contentful paint)\n4. Build Time - the amount of time it takes to build and deploy the website is slower for SSG and ISR since we need to build all the pages to serve where as CSR and ISR build pages dynamically.\n\n##### References\n\n1. https://theodorusclarence.com/blog/nextjs-fetch-usecase\n","n":0.043}}},{"i":11,"$":{"0":{"v":"History of Front-End","n":0.577},"1":{"v":"\n## 1989 \nBirth of World Wide Web created by Tim Berners-Lee.\n\n## 1990-1993 - Web 1.0:\n\n  - [First web page](http://info.cern.ch/hypertext/WWW/TheProject.html) is written in HTML and deployed by Berners-Lee and hosted on his NeXT computer. \n  - 1993 -  Tim Berners-Lee writes first version of HTML\n  -  HTMLs `<table>` was the only way to organize a page. No colour blocks, no images, no graphics, just text\n  - web pages had no dynamic content \n  - JS/CSS still in infancy\n  - Major browser players were Netscape and IE\n\n## 1993 - 1999 - Dynamic websites\n  - 1993 -  Common Gateway Interface (CGI) was created allowing webservers to execute programs/scripts as opposed to returning html from a file system.  \n  - 1994 - PHP conceieved and grew in popularity over the years\n  - 1995 - JavaScript is born \n  - 1996 - CSS became available\n  - Web animation introduced with Macromedia Flash (1996)\n  - Websites were only created for desktop browsers (non-responsive)\n\n## 2000s - Web 2.0:\n  - Dynamic HTML - css/js manipulates DOM\n  - Drag n Drop animations\n  - Amazon, Wikipedia, Youtube\n  - Server side vs Client side rendering emerging:\n    - things like myspace - SSR meant that you'd have to refresh the page in order to get a notification as logic existed in the backend\n  - 2006 - JQuery\n\n  \n\n## 2000-2006\n  - Responsive design was trending\n  - Business purposed websites and blogging were trending \n  - As a result CMS' (around since 1990s) were gaining more popularity\n  - WordPress launched in 2003 - still powers around 35.9% of the internet.\n  - Myspace (2003) and Facebook (2004) launched. \n  - Facebook goes public and gains popularity in 2006\n\n\n## Mobile Era (2007-2010)\n  - 2007 - iPhone released \n  - Safari launched - first fully usable HTML browser on a phone\n  - iPhone did not support Flash which influenced it's eventual demise. \n  - Mobile friendly design became a necessity\n\n## 2010 - present\n\n// @todo\n\n\n\n\nReferences:\n1. https://www.youtube.com/watch?v=Kzeog8yTFaE\n2. https://dev.to/snickdx/a-brief-history-of-the-web-9c3\n3. https://tillerdigital.com/blog/the-history-of-web-design/\n","n":0.056}}},{"i":12,"$":{"0":{"v":"Design Systems","n":0.707},"1":{"v":"\nDesign Systems exist to create a source of truth for a product design. Some notable design systems:\n\n1. Google's [Material UI](https://material.io/)\n2. [Atlassian](https://atlassian.design/)\n3. IAG's [Chroma](https://chromadesignsystem.com/)\n\n### Design Tokens\n\nDesign tokens are a way to store styles so that they can be re-used by any platform e.g. web or mobile\n\n#### References\n\n1. https://zure.com/blog/collaboration-with-design-tokens-and-storybook/\n","n":0.146}}},{"i":13,"$":{"0":{"v":"Frameworks","n":1},"1":{"v":"\nGeneral notes about various new/hot JS frameworks and their differences\n\n## Rendering\n\n**React** - starts from the top and any update to state will re-renders all the way down the component tree\n\n**Solid** or **Qwik**: Change in state is not associated with a component rather directly with a DOM node.\n\n## Pre-rendering\n\n**React and Solid** : Hydrate the html sent to the client\n\n**Qwik** : Serialize the reactivity graph at the server, effectively lowering time to interactive (TTI)\n\n## Interactivity\n\n**React**: All components re-execute on state change from interactivity\n**Solid** or **Qwik**: Only execute at the event handle\n\nImagining a simple app where Counter is the parent and Display/Increment are children that display and update parent state:\n![https://www.builder.io/blog/resumability-vs-hydration?ck_subscriber_id=1441143038#client-side-rendering](/assets/images/2023-02-13-20-59-56.png)\n\n## Resumability\n\nQwik uses 'resumability' as opposed to hydration which is a way to recover state without re-executing the application components on the client. This is done by serializing everything required e.g. html and event listeners, for the client to resume where the server left off.\n","n":0.081}}},{"i":14,"$":{"0":{"v":"Backend","n":1}}},{"i":15,"$":{"0":{"v":"Database","n":1},"1":{"v":"\n## Types\n\n### Relational (SQL)\n\nInformation is stored in tables, in rows and columns. Tables usually have relationships with other tables.\n\nStructured Query Language (SQL) is used to interact with relational databases.\n\nExamples:\n\n- Microsoft SQL Server\n- MySQL\n- PostgresSQL\n\nAdvantages of using a relational database include:\n\n**ACID Compliance**\n\nAtomicity, Consistency, Isolation, Durability (ACID) is a standard that guarantees the reliability of database transactions. ACID transactions are a group of database operations that succeed only if all the operations within succeed. This ensures that the database remains in a consistent state and there are no dangling incomplete mutations.\n\nTo understand this, we can think of a bank transfer where we transfer from one account to another. Here we expect that in our first account, we expect both accounts to reflect a new transaction e.g. $100 from ACC1 to ACC2:\n\n```SQL\nUPDATE bankaccounts SET funds=funds-100 WHERE accountno='ACC1';\nUPDATE bankaccounts SET funds=funds+100 WHERE accountno='ACC2';\n```\n\nIf our second statement were to fail, the accuracy of the database would be compromised. In order to fix this, we use a transaction:\n\n```SQL\nSTART TRANSACTION or BEGIN; -- Wrap the statements in a transaction\nUPDATE bankaccounts SET funds=funds-100 WHERE accountno='ACC1'; --statement2\nUPDATE bankaccounts SET funds=funds+100 WHERE accountno='ACC2'; --statement3\nCOMMIT; -- COMMIT to DB once all statements pass successfully\n```\n\n**Data Accuracy**\n\nPrimary and foreign keys ensure there is no duplication.\n\n### Non-relational (No SQL)\n\n### Relational or Non-relational?\n\n#### References\n\n1. https://www.softwaretestinghelp.com/mysql-transaction-tutorial/\n2. https://www.mongodb.com/basics/acid-transactions\n3. https://www.mongodb.com/compare/relational-vs-non-relational-databases\n","n":0.069}}},{"i":16,"$":{"0":{"v":"API Architecture","n":0.707},"1":{"v":"\nA note describing some of the different API architectures/protocols, namely RPC, REST and GraphQL.\n\n## RPC - Remote Procedure Call\n\nEarliest API architecture. Remote Procedure Call (RPC), as the name implies, calls a remote \"[procedure](programming-paradigms#procedural-programming,1:#*)\" or a function e.g. from a client application to the API on a server.\n\nHow the procedure/function is invoked on the server depends on the framework and choice of technology adopted. RPC is just a concept, and in a simple scenario, the method called could be determined by simply parsing query parameters from the url coming from the client and performing a string comparison, matching the method from the client to the server.\n\nA contrived example where a client invokes the API to perform mathematical operations on two numbers 'a' and 'b' and return the result:\n\n```Javascript\n/** Server API **/\nconst express = require('express')\nconst app = express()\nconst port = 3000\n\napp.get('/',(req, res) => {\n  // extract the method being called from the query params\n  // URL is in the format http(s)://<host>/method=<methodName>&b=<b>&c=<c>\n  const { method } = req.query;\n  const a = parseInt(req.query.a);\n  const b = parseInt(req.query.b);\n\n  let methodResponse = \"\";\n\n  // call the method requested - here we do a simple string comparison\n  // so that it appears the FE and BE \"share\" procedures\n  switch(method) {\n     case \"add\":\n     \tmethodResponse = add(a,b);\n        break;\n     case \"multiply\":\n     \tmethodResponse = multiply(a,b);\n        break;\n     case \"minus\":\n     \tmethodResponse = minus(a,b);\n\tbreak;\n     default:\n        methodResponse = \"method doesn't exist\";\n\n  }\n\n  res.send({ result: methodResponse });\n})\n\nfunction add(a, b) {\n  return a + b;\n}\n\nfunction multiply(a, b) {\n  return a * b;\n}\n\nfunction minus(a, b) {\n  return a - b;\n}\napp.listen(port, () => {\n  console.log(`Example app listening on port ${port}`);\n})\n\n```\n\nA client can communicate to the above API application as follows:\n\n```bash\nGET http://localhost:3000/?method=multiply&a=4&b=2\n{output: 8} #response recieved\n\n```\n\n### Disadvantages\n\n**Tight coupling** - RPCs client and server are tightly coupled since function calls need to match.\n\n**Leaking internal implementation** - It may be considered a flaw that the inner workings of the API are somewhat revealed to the client as the backend method names may be \"telling\".\n\n**Extensibility of methods is difficult** - it's often the case that in RPC API's, a procedure may not perform the exact functionality and hence another method that does something slightly different is added. In this case, there is an overload of similar functions which can be messy:\n\n```javascript\nfunction multiply() {\n  // does something\n}\n\nfunction multiplyV2() {\n  // does something similar to multiply() but different\n}\n\nfunction multiplyV3() {\n  // etc\n}\n```\n\n### Advantages\n\n**Lightweight** - RPC APIs are expected not to return a lot of meta data and the most popular frameworks such as gRPC are built in such a way that allows them to be more performant than their REST counterpart.\n\n**Action-oriented** - RPCs are good for action oriented applications where we are calling 'commands' rather than focusing on resources.\n\n### gRPC\n\nOne of the most notable examples of RPC frameworks in modern day is [gRPC](https://grpc.io/). gRPC is an open-source Google framework used for high performance microservice architecture styles. It's supported by multiple languages/frameworks such as Go, Node, Rust Python and more. It's used by many well known tech companies such as Square, Netlfix and [Cloudflare](https://blog.cloudflare.com/road-to-grpc/).\n\ngRPC is considered highly performant due to:\n\n- Using HTTP/2 for transport\n- Allowing both clients and servers to stream data as opposed to polling (_Note: I believe this is available due to HTTP/2_)\n- Encouraging **Protobuf** as the messaging format\n\nProtobuf (protocol buffers) are a message format that allow for serialization of structured data in a language neutral way. They are an alternative messaging format to others such as JSON or XML and are considered more compact to achieve low latency.\n\nExample of a Protobuf message:\n\n```Protobuf\n// Person.proto\nmessage Person {\n  required string name = 1;\n  required int32 age = 2;\n  optional string email = 3;\n}\n```\n\nProtobufs are encoded in such a way that makes them lightweight and performant.\n\n![](/assets/images/2022-03-03-10-51-00.png)\n\n<caption>Source: https://developers.google.com/protocol-buffers/docs/overview#work</caption>\n<br>\n<br>\n\ngRPC can be implemented using a variety of languages or frameworks, however it seems performance varies - multiple language performance tests are [performed often](https://grpc.io/docs/guides/benchmarking/) on the code base. Note that the [performance dashboard](https://grafana-dot-grpc-testing.appspot.com/?kiosk=tv) is missing Node however, it has been [benchmarked by 3rd parties](https://www.nexthink.com/blog/comparing-grpc-performance/) and _appears_ to be less performant than alternatives such as Go.\n\ngRPC currently cannot natively communicate with the web via HTML/2. Browsers do not support protobuf payloads and hence, grpc-web was introduced in order to act as a proxy to serialize web payload and give developers the ability to use protobufs across the full stack:\n\n![https://grpc.io/blog/state-of-grpc-web/](/assets/images/2022-03-03-10-13-17.png)\n\n<caption>Source: https://grpc.io/blog/state-of-grpc-web/</caption>\n\n</br>\n</br>\nPrior to this, developers would likely use REST APIs with HTTP to communicate with the gRPC backend.\n\n## REST\n\n**Re**presentational **s**tate **t**ransfer (REST) API is an architectural style created by Roy Fielding, described famously in his [dissertation](https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm).\n\nREST is commonly not adhered to when implemented as many APIs do not tick the architectural constraints that make an API \"RESTful\". There are **6 constraints** which make a web service truly RESTful:\n\n1. **Uniform Interface**\n\nWhat distinguishes REST is uniform interface whereby the client and server are decoupled in such a way that allows them tfcv o evolve independently. Having this interface provides the client with enough information to retrieve the relevant data without having to understand the underlying implementation. This concept is achieved through these four principles:\n\n- **Identification of resources** - where RPCs are command or action oriented, REST APIs are modeled around resources. For example, users in a rest API are handled via the URI `/user`\n\n- **Manipulation of resources through representations** - When a client holds a representation of a resource, including any metadata attached, it has enough information to modify or delete the resource on the server. This makes REST APIs **discoverable** in that from the root of the API, the client has enough information to perform any available action on the available resources.\n\n- **self-descriptive messages** - Each message includes enough information to describe how to process the message. For example, by using standard [MIME Types](https://en.wikipedia.org/wiki/Media_type#cite_note-10) e.g. `application/json`, . Responses also explicitly indicate their cache-ability. e.g. `Cache-Control: max-age=31536000`\n\n- **hypermedia as the engine of application state (HATEOAS)** - allows the client to dynamically navigate to the appropriate resources by traversing the hypermedia links. For example, a call to `GET http://localhost:8080/users` could return:\n\n```JSON\n\n\n```\n\n#### References\n\n1. https://blog.jscrambler.com/rpc-style-vs-rest-web-apis\n2. https://grpc.io/\n3. https://www.restapitutorial.com/lessons/whatisrest.html#\n","n":0.032}}},{"i":17,"$":{"0":{"v":"Auth","n":1},"1":{"v":"\nAuth options:\n\nSupabase\n\nNextAuth\n\nAuth0\n","n":0.707}}},{"i":18,"$":{"0":{"v":"Reactive Programming","n":0.707},"1":{"v":"\n**Reactive Programming** is a design paradigm that is programming with asynchronous data streams. These data/event streams are observable and can be used to trigger \"side-effects\". A simple example of this is a click event. In an application we can fire effects upon the trigger of a user click by observing that event.\n\n[The Gang of Four](https://en.wikipedia.org/wiki/Design_Patterns) famously defined two patterns that became fundamental to the focus of reactive programming:\n\n- Iterator - the consumer pulls data out of the producer (imperative - consumer is in control)\n- Observer - a way for a producer to give the consumer one item at a time (declarative - producer is in control)\n\nThe book misses what is the correspondence between these two patterns: The iterator pattern allows you to pull data out and the producer lets the consumer know when there is no more data AND if an error occurred. These two semantics were left out of the observer pattern by the GoF.\n\nReactive programming is about unifying the observable type with the iterator type and therefore produces this new type known as the \"observable\". The benefit of this is now we can react in the way we do to static data to dynamic events in real time.\n\nBy adding these two completion semantics of the two patterns, we can now use all the ways we know when it comes to how to transform streams of data that we can pull on the streams that we pull e.g. map, filter, reduce, zip, merge etc.\n\nThe advantage of this is now event data does not need to be stored anywhere when it comes to processing. Meaning we don't need to have big data stores that we pull from in order to process data - we can take the data as it arrives, process it and be on our way. We allocate less memory this way. If you can write a sql query to transform data in a temporary table somewhere, you can now write a sql query over live data. It's all about serving the consumer faster. [^3]\n\n#### References:\n\n1.  https://gist.github.com/staltz/868e7e9bc2a7b8c1f754\n2.  https://dev.to/this-is-learning/what-the-hell-is-reactive-programming-anyway-31p5\n3.  [Reactive Programming Overview (Jafar Husain from Netflix)](https://www.youtube.com/watch?v=-8Y1-lE6NSA)\n\n[^1]: Quote from https://dev.to/this-is-learning/what-the-hell-is-reactive-programming-anyway-31p5\n","n":0.053}}},{"i":19,"$":{"0":{"v":"Reactive Programming In Java (Course Notes)","n":0.408},"1":{"v":"\n> These notes are based entirely off the course: [\"Reactive Programming with Java\"](https://www.youtube.com/watch?v=OiRx2pZskR0&list=PLqq-6Pq4lTTYPR2oH7kgElMYZhJd4vOGI&index=5)\n\nThe default programming module in java is known as the blocking programming module. We make a request, it takes time to process, and then it responds. The alternative to this is what is known as Reactive Programming.\n\nIn Java, there are several frameworks that do this, namely:\n\n- Reactor - has the maximum momentum of all the projects and is integrated with spring boot. A lot of cloud providers use it e.g. azure for their apis\n- RxJava\n\nA lot of people confuse reactive with async. Reactive programming doesn't imply it's async as it can by synchronous. A typical or traditional use cases of reactive programming are:\n\n- user events\n- I/O responses e.g. file is completed\n\nReactive Programming is not exactly event driven programming.\n\nWhy do we care:\n\nWhen we think about modern application development, there are things that stand out today:\n\n1. high data scale\n2. high usage scale e..g. lots of users, availability\n3. cloud based costs - when servers were on prem it did not matter as much as cloud as inefficiencies can cost a lot\n\nBefore doing vertical or horizontal scaling, it's often important to optimize the code.\n\nGiven Java is non-blocking, Reactive programming can be seen commonly in JavaScript e.g. Node JS with Promises\n\nImagine a web application where you have a request from a user to retrieve preference:\n\n```Java\n@GetMaping(\"/users/{userId}\")\npublic User getUserDetails(@PathVariable String userId) {\n    User user = userService.getUser(userId);\n    UserPreferences prefs = userPreferenceService.getPreferences(userId);\n    user.setPreferences(prefs);\n    return user;\n}\n```\n\nThis endpoint makes a call to two services which are run sequentially as opposed to in parallel i.e. `userService` call to `getUser` must complete before `getPreference` is triggered via the `userPreferenceService`. This becomes a problem when we consider the requests as spinning up new threads for each user. Given these two services are not dependant on each other, the longevity of the request takes longer and the result is more threads are alive at any given time than necessary. The issue is that we now have wasted resources/hardware.\n\nTraditionally, Java developers don't think about concurrency as spring boot or mvc abstracts and handles it for them. When writing APIs, the mentality of the developer is that this code is handling a single request. Methods are written in a stateless way so each request does not touch another. The cost to this becomes:\n\n1. sequential blocking operations\n2. idling threads\n\nReactive programming is not the only solution. There are concurrency APIs in Java which can handle this. For example, doing the previous `getUserDetails` blocking section using concurrency APIs:\n\n```Java\nCompletableFuture<User> userAsync = CompletableFuture.supplyAsync(() -> userService.getUser(userId));\nCompletableFuture<UserPreferences> userPreferencesAsync = CompletableFuture.supplyAsync(() -> userPreferenceService.getPreferences(userId));\n// combine into a single future\nCompletableFuture<Void> bothFutures = CompletableFuture.allOf(userAsync, userPreferencesAsync)\n// block until they're ready\nBothFutures.join();\n// get the results and put into user\nUser user = userAsync.join();\n// get the results and put into prefs\nUserPreferences prefs = userPreferencesAsync.join();\nuser.setUserPreferences(prefs);\nreturn user;\n```\n\nThis works however it's still causing a thread to idle and that is due to `bothFutures.join()` which, although allows the two services to be called in parallel, still blocks until they are complete. The other problem is this that Future API like the example above is very messy and a lot to work with. This is what Reactive Programming aims to resolves.\n\nThe above example, using Reactive Programming, becomes:\n\n```Java\npublic Mono<User> getUserDetails(@PathVariable String userId) {\n    return userService.getUser(userId)\n        .zipWith(userPreferencesService.getPreferences(userId))\n        .map(tuple -> {\n            User user = tuple.getT1();\n            user.setUserPreferences(tuple.getT2);\n            return user;\n        })\n}\n```\n\nThe difference above to the concurrent approach is:\n\n- much simpler\n- Reusable flexible functions - many of the paradigms can be re-used and applied all over the application\n\nReactive programming is more declarative than imperative. Java has a 'Flow' interface that can be used by reactive programming libraries (in reality it was not adopted as much as was hoped).\n\nReactive programming is not worth it for small projects. The learning curve is big and the benefit only shows when the size of the project is increased.\n\nReactive programming in java may seem familiar when comparing to Java Collections. Java Streams represent a sequence of data, with no concern over the internal representation of the collection, we just focus on the computation of the data. The iteration is also internal and is obfuscated from the programmer.\n\n```Java\nList<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);\n\n// Traditional loop\nfor (int i = 0; i < numbers.size(); i++) {\n    System.out.println(numbers.get(i));\n}\n\n// Streams\nnumbers.stream().forEach(number -> System.out.println(number))\n```\n\nTypical stream operations: map, filter, flatMap, findFirst and more.\n\nRelevant Design patterns:\n\n- Iterator Pattern - traverse a container and access it's elements. It decouples the algorithms from the container meaning that we don't care about the underlying implementation and that the iterator can take any container and gives us a consistent experience (Collections).\n- Observer Pattern - used to observe an event/number of events and trigger a course of action.\n\nThese two patterns are sort of inverses of each other (push/pull data).\n\n```Java\n// iterator\nmyList.forEach(element -> System.out.println(element)); //push\n// observer\nclicksChannel.addObserver(event -> System.out.println(event)) //pull\n```\n\nThe idea these two use the same module/paradigm is the fundamental idea behind reactive programming.\n\n**Reactive Streams** were introduced in Java 9.\n","n":0.035}}},{"i":20,"$":{"0":{"v":"React","n":1},"1":{"v":"\n## Release 18 Features\n","n":0.5}}},{"i":21,"$":{"0":{"v":"React v18.0","n":0.707},"1":{"v":"\n## Concurrent Rendering\n\nA behind-the-scenes mechanism introduced in React 18 in order to render more seamlessly.\n\nBefore React 18, rendering was a synchronous transaction meaning that once rendering started it could not be interrupted. React 18 introduces features such as suspense and stream server rendering meaning rendering can be interrupted, paused, resumed or abandoned.\n\nThe problem with synchronous rendering is that while a component is re-rendering, the rest of the app becomes unresponsive until it's complete since main thread is blocked. If we picture this being a list that is updated each time the user changes input and the list takes a while to re-render, the other components must wait until it completes making it a janky user experience.\n\n## Automatic Batching\n\n## Transitions\n\n## New Suspense Features\n\n## New Client and Server Rendering API\n\n## New Strict Mode Behaviors\n\n## New Hooks\n","n":0.086}}},{"i":22,"$":{"0":{"v":"Projects","n":1}}},{"i":23,"$":{"0":{"v":"Operation Rewrite","n":0.707},"1":{"v":"\n## Project: Operation Re-write (Purposely obscure)\n\n### Problem:\n\nExisting site is slow and difficult to improve given it's built in wordpress. Gives limited options for improvement from a developer perspective without adding more bloat and plugins. Owned by non-for-profit therefore budget is an important consideration.\n\n### Solution:\n\nRe-write using headless CMS and following JAMSTACK princples.\n\n### Existing stack\n\n- Frontend: PHP\n- Backend: PHP\n- CMS: Wordpress\n- DB: SQL\n\n### Proposed re-architected stack:\n\n- Frontend: React framework\n- Backend - possibly NodeJS with GraphQL\n- CMS: Headless\n- DB: ?? not sure if I need one\n\n### Rendering Style\n\nRef [[web.frontend.rendering-strategies]]\n\nOptions: SSR, SSG, SPA, ISR\n\nContent is mostly static - new blog posts written relatively infrequently. There is an ecommerce section with a small number of products. No comments or anything else that is dynamic at the moment.\n\nSSG could be a good option here but a framework that supports SSR as well could be useful for any future concerns. Maybe best of both worlds and use ISR? No need to rebuild every time we change a post or add a product.\n\n### Framework Options\n\n- Remix - looks good but pretty new\n- Next - established, but maybe bloated for my use case? Needs a server to run for ISR and SSR\n- Gatsby - only does SSG (?)\n\n### Headless CMS Options\n\n- Strapi - free/open source\n- Contentful - recommended but paid after certain point\n\n### DB?\n\nDo I need?\n\n- PSQL\n\n## Build steps\n\n1. Set up CMS\n2. Set up FE\n3. interface the two\n","n":0.066}}},{"i":24,"$":{"0":{"v":"Programming Paradigms","n":0.707},"1":{"v":"\n**Programming paradigm refers to a style of programming.** Programming languages need to follow some strategy when they are implemented known as a paradigm.\n\nThere are two significant paradigms programming languages fall into - **Imperative** and **Declarative**. These categories have various subtypes:\n\n```mermaid\ngraph TD;\n    D[Imperitive]-->P[Procedural];\n    D --> O[Object Oriented]\n```\n\n```mermaid\ngraph TD;\n    D[Declarative]-->L[Logic];\n    D --> F[Functional]\n```\n\n## Imperative\n\nThe word “imperative” comes from the Latin “impero” meaning “I command”. This is the oldest programming paradigm. It effectively means the code is a series of commands executed in order to achieve a result.\n\nImperative tells a program exactly how to do something. For example, if we need a program to print numbers in an array that are less than 10:\n\n```Javascript\n    const nums = [5, 10, 15, 8, 7]\n    let lessThanTen = []\n\n    for(i = 0; i < nums.length; i++) {\n        if(nums[i] < 10) {\n            lessThanTen.push(nums[i]);\n        }\n    }\n\n    // prints [5, 8, 7]\n    console.log(lessThanTen)\n```\n\n## Declarative\n\nDeclarative programming focuses on the end result as opposed to how to get there. For example, if we need a program to print numbers in an array that are less than 10:\n\n```Javascript\n    const nums = [5, 10, 15, 8, 7]\n    // prints [5, 8, 7]\n    console.log(nums.filter(num => num < 10))\n```\n\nReact is a declarative framework as rather than telling it how to render (in the same way you do with vanilla JS), you tell it what you want it to render.\n\n## Types of Imperative Programming\n\n### Procedural Programming\n\nBuilt around the idea that programs are sequences of instructions to be executed - splits up programs into 'procedures' or functions. This is effectively the same as imperative with the idea of using code splitting and enhancing reusability with subroutines.\n\n```Javascript\n    // use a function (procedural)\n    function getNumbersLessThanTen(nums) {\n        let lessThanTen = []\n        for(i = 0; i < nums.length; i++) {\n            if(nums[i] < 10) {\n                lessThanTen.push(nums[i]);\n            }\n        }\n        return lessThanTen\n\n    }\n\n\n    // prints [5, 8, 7]\n    console.log(getNumbersLessThanTen([5, 10, 15, 8, 7]))\n```\n\n### Object-oriented Programming\n\nOOP was introduced in order to overcome procedural programming limitations. In OOP, code is organised into objects and classes. In conventional procedural programming, data was global and there was a difficulty in securing the program from unintended changes. OOP controls access to data via encapsulation within classes.\n\nThe main priniciples of OOP are:\n\n1. Encapsulation\n2. Inheritance\n3. Abstraction\n4. Polymorphism\n\n## Types of Declarative Programming\n\n### Logical programming\n\nMade up of facts and rules and foundationally mathematical. It takes a declarative approach to problem solving by using everything it knows and trying to answer whether the given facts and clauses are true. For example, if the code states that A is equal to B and B is equal to C, a logical program will make the logical conclusion that A must be equal to C.\n\nExamples of languages that use logic programming paradigm are Prolog, Absys or Alice.\n\n### Functional programming\n\nFunctional programming is language independent. FP is the process of building software by composing pure functions, avoiding shared state, mutable data and side effects. JavaScript is a notable example of a language that fundemnatally supports the functional paradigm.\n\nA pure function is one which, given the same inputs, always returns the same output and has no side effects. This is an important aspect of functional programming. The idea is that functions are not dependent on local or global state.\n\nFunctions are described as \"first class objects\" in functional programming. The reason is that functions can be assigned to variables, passed as arguments, and returned from other functions, just as any other data type can.\n\n```JavaScript\n\n// assign function to variable!\nlet log = function(someVariable) {\n    console.log(someVariable);\n    return someVariable;\n}\n\n\nlet myVar = \"Hello world\";\n\nlog(myVar);\n// Hello world\n```\n","n":0.042}}},{"i":25,"$":{"0":{"v":"Nextjs","n":1},"1":{"v":"\n## Post 14.0 notes and usage\n\n### Routing\n\nIn version 13, Next.js introduced **App router**. Prior to this, Next used page router.\n\nThe difference from the outset means that prior to 13, users would place their routes within a `pages/` directory where each file is considered a route:\n\n```\n└── pages\n    ├── about.js\n    ├── index.js\n    └── team.js\n\n```\n\nWith 13 onward, app router looks for a `pages.(js|ts|tsx)` file who's route is determined by it's parent folders. Eg:\n\n```\n\n└── app\n    ├── about\n    │   └── page.js\n    ├── globals.css\n    ├── layout.js\n    ├── login\n    │   └── page.js\n    ├── page.js\n    └── team\n        └── route.js\n\n```\n\nSubfolders (leaves) become nested routes.\n\nApp router is the recommended approach in modern NextJS projects. Page router is still supported.\n\nApp router is built on **React Server Components** (RSC) By default, components inside app are RSC components.\n\n### File conventions\n\nNextjs has various reserved file names that provide common inbuilt routing functionality:\n\n- `layout.js` - a shared segment between pages that preserves states, remains interactive and doesn't re-render. The top most layout is known as the **_Root layout_** and it is required component that is shared by all pages in the application. It must contain a html and a body tag.\n- `template.js`\n- `error.js` - handles errors in routes\n- `loading.js` - creates a loading view. Beneath the hood it's tied in with Suspense in the layout file.\n- `not-found.js` - view for an unknown route\n- `page.js` or nested `layout.js`\n\n### Data Fetching\n\n#### On the server\n\nBy default, Next.js automatically caches the return values of `fetch` in a built-in **Data Cache** on the server.\n\nIf data is dynamic or requires re-validation, cached data needs to be purged. There are two ways to re-validate:\n\n- Time based - revalidate once a certain amount of time has passed. This is useful for data that doesn't change frequently and freshness is not critical. This is good for a mostly static website.\n\n```js\nfetch(\"https://...\", { next: { revalidate: 3600 } });\n```\n\n- On-demand revalidation - manually revalidate data based on an event. This is on par with traditional server side rendering to ensure that data is always fresh on every request.\n\nIf you want a request to opt-out of caching, you need to set a `no-store` option in the fetch call:\n\n```js\n// Opt out of caching for an individual `fetch` request\nfetch(`https://...`, { cache: \"no-store\" });\n```\n\nAlternatively, you can use the route segment config option to opt out of caching for a specific route segment:\n\n```js\n// Opt out of caching for all data requests in the route segment\nexport const dynamic = \"force-dynamic\";\n```\n\n#### On the client\n\nFetching data on the client can be done via a Route Handler or a third party library such as Vercel's [SWR](https://swr.vercel.app/) or [TanStack Query](https://tanstack.com/query/latest).\n\nRoute Handlers execute on the server and therefore protect sensitive information.\n\nRoute handlers replace the API routes from 13.0 onward (although API routes can still be used).\n\n### Data fetching recommendations\n\nNext.js recommends that whenever possible, to fetch data on the server with server components. This is done via server actions.\n\nServer actions are react functions that are asynchronous and are executed on the server. They can be denoted via the \"use server\" directive, can be inline or within a separate file.\n\n```jsx\n// Server Component\nexport default function Page() {\n  // Server Action\n  async function create() {\n    'use server'\n\n    // ...\n  }\n\n  return (\n    // ...\n  )\n}\n```\n","n":0.044}}},{"i":26,"$":{"0":{"v":"Machine Setup","n":0.707},"1":{"v":"\nMy machine setup preferences.\n\n## Programs\n\n1. [Steermouse](https://plentycom.jp/en/steermouse/) - to set up mouse hot keys\n2. [Rectangle](https://rectangleapp.com/) - Mac window snapping\n3. [kitty](https://sw.kovidgoyal.net/kitty/) - better terminal\n4. [bat](https://github.com/sharkdp/bat) - instead of `cat` for syntax highlighting and pretty print\n\n5. [tmux](https://github.com/tmux/tmux/wiki) - terminal multiplexer\n6. [vscode](https://code.visualstudio.com/) - for most languages and frameworks unless I need a really smart IDE then I would use intellij\n7. [oh-my-zsh](https://ohmyz.sh/) - zsh config manager\n8. [jq](https://stedolan.github.io/jq/) (optional, I only use this sometimes) - pretty printing json usually to pipe curl output\n\n## Configuration files\n\n### vim\n\n```bash\nfiletype plugin indent on  \" Load plugins according to detected filetype.\nsyntax on                  \" Enable syntax highlighting.\n\ncolorscheme desert\nset autoindent             \" Indent according to previous line.\nset expandtab              \" Use spaces instead of tabs.\nset softtabstop =4         \" Tab key indents by 4 spaces.\nset shiftwidth  =4         \" >> indents by 4 spaces.\nset shiftround             \" >> indents to next multiple of 'shiftwidth'.\nset number\nset listchars=eol:¬,tab:>·,trail:~,extends:>,precedes:<\nset list\nset showmode               \" Show current mode in command-line.\n\nset incsearch              \" Highlight while searching with / or ?.\nset hlsearch               \" Keep matches highlighted.\n\nset cursorline             \" Find the current line quickly.\n```\n\n## tmux\n\n```bash\nunbind C-b\nset-option -g prefix C-a\nbind-key C-a send-prefix\n\n# split panes using | and -\nbind | split-window -h\nbind - split-window -v\nunbind '\"'\nunbind %\n\n# reload config file (change file location to your the tmux.conf you want to use)\nbind r source-file ~/.tmux.conf\n\n# Enable mouse mode (tmux 2.1 and above)\nset -g mouse on\n\nset -g history-limit 5000\n\nset -g default-terminal \"screen-256color\"\n\n```\n\n## zhrc\n\n```bash\n# keep most of the file with some changes/ additions\n\nZSH_THEME=\"alanpeabody\"\n\nplugins=(\n    git\n     zsh-autosuggestions\n)\n\n## command line prompt -\n## shows username, path and git branch details\nparse_git_branch() {\n    git branch 2> /dev/null | sed -n -e 's/^\\* \\(.*\\)/[\\1]/p'\n}\nCOLOR_DEF='%f'\nCOLOR_USR='%F{243}'\nCOLOR_DIR='%F{197}'\nCOLOR_GIT='%F{39}'\nNEWLINE=$'\\n'\nsetopt PROMPT_SUBST\nexport PROMPT='${COLOR_USR}%n ${COLOR_DIR}%d ${COLOR_GIT}$(parse_git_branch)${COLOR_DEF}${NEWLINE}%% '\n\n## default editor\nexport EDITOR=vim\n\n# kitty - Fix issues with terminal navigation in tmux\nbindkey \"\\e[1;3D\" backward-word # ⌥←\nbindkey \"\\e[1;3C\" forward-word # ⌥→\n\n\n# for JQ extention if using -  \"null\" is hard to see so change the colour\nexport JQ_COLORS='0;31:0;39:0;39:0;39:0;32:1;39:1;39'\n```\n","n":0.058}}},{"i":27,"$":{"0":{"v":"Data structures & Algorithms","n":0.5}}},{"i":28,"$":{"0":{"v":"Leetcode","n":1},"1":{"v":"\n### Leetcode solutions + explanation\n\n\n### 53.Maximum Subarray \n\nhttps://leetcode.com/problems/maximum-subarray/\n\n#### Approach\nIn order to get the maximum sub array, we keep track of the maximum sum so far and a running sum of the current subarray.\n\nWe iterate over the array from the 2nd index, with both current and max sum set to the first value in the array. We need to determine whether we consider the current number as part of this sub array or if we should start a new window. How do we know if continuing this subarray or not is the right choice? We check whether the sum so far PLUS the current number is better than just taking the number itself. This is to cater for the scenario where you have negative numbers in a subarray. Consider the scenario where subarray is either good to continue vs starting new:\n\na) [4, -1, 8] \n\nvs\n\nb) [4, -6, 8]\n\nIn a) when we get to the number 8, currentSum will equal 3 (4+(-1)). We then check whether currentSum + 8 is better than 8 itself. In this scenario, the sum is better (3+8 > 8) therefore we take the whole array and max sum becomes 11\n\nif we take b), when we get to 8, the current sum is -2 (4+(-6)). If we compare the currentsum + 8 to just 8, it's more beneficial to start a new window (at 8) since (-2 + 8 < 8). Therefore max sum in the second scenario becomes 8.\n\n#### Solution\n\n**Time**: `O(n)`\n\n**Space**: `O(n)`\n\n```js\n/**\n * @param {number[]} nums\n * @return {number}\n */\nvar maxSubArray = function(nums) {\n    let maxSum = nums[0];\n    let currentSum = nums[0];\n\n    for(let i = 1 ; i < nums.length; i++) {\n        currentSum = Math.max(currentSum + nums[i], nums[i]);\n        maxSum = Math.max(currentSum, maxSum)\n    }\n\n    return maxSum;\n};\n\n```","n":0.059}}},{"i":29,"$":{"0":{"v":"Architecture","n":1},"1":{"v":"\nTopics that don't necessarily fit into just FE or BE e.g. repo management, CI/CD, devops.\n\n## Types of Architectures\n\n### Monolithic Architecture\n\nA single server application\nresponsible for all the features of the system e.g. a hotel application could have a single application taking care of payments, bookings, campaign offers and so on. This is a common way for applications to start out and even continue if they are simple or non-complex.\n\n### Microservices Architecture\n\nSplitting out responsibility into multiple application servers e.g. one for payments, one for streaming, one for recommendations.\n\nThis allows us to scale individual services and prevent code from becoming too complex by making use of single responsibility principles and loose coupling.\n\nMicroservices architecture is more complex to set up and manage and is therefore more useful for larger projects.\n\n## Patterns and Design Principles\n\n### Command Query Responsibility Segregation (CQRS)\n\nA pattern that separates the responsibility of reading (querying) data and modifying (commanding) the data. On the command side, CQRS emphasizes the use of commands to perform action that modify application state. The traditional alternative to CQRS is CRUD (create, read, update delete) where the same services are responsible for the handling of all operations.\n\nOn the query side, CQRS emphasizes the use of queries to retrieve data from the system. These are typically handled by query handlers within the system.\n\nThe main advantage of separating command and query is giving developers the ability to optimize each side independently. Read can be optimized with strategies such as caching or specialized data stores. CQRS is useful in complex domains or applications where read and write have different scalability and performance requests. Considering it adds complexity would mean that it's not suitable for all requirements and trade-offs should be taken into account.\n\n[[Read more|architecture.CQRS]]\n\n### ACID & CAP Theorem\n\nACID (Atomic, Consistency, Isolation, Durability) and CAP (Consistency, Availability, Partition Tolerance) are concepts in distributed systems that are used to explain the trade offs between consistency and availability.\n\nAccording to the CAP theorem, any distributed system can only guarantee two of three properties at a time. You can't guarantee all three properties at once:\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Br1FrvKnK3hU6Xl_LbDkwg.png\">\n\nACID is a set of properties of database transactions intended to guarantee validity even in the event of errors, power failures, etc.\n","n":0.053}}},{"i":30,"$":{"0":{"v":"Tech Fundamentals","n":0.707},"1":{"v":"\nNotes on Technical Fundamentals course (https://learn.cantrill.io/p/tech-fundamentals) - summary of technical concepts considered a precursor to aws certification / general tech architecture.\n\n## OSI Model\n\nThe Open Systems Interconnection (OSI) model is a conceptual framework that describes the communication functions of a telecommunications or computer system. It is divided into seven layers each of which performs a specific function in the transmission of data between networked devices. This is sometimes known as the Networking Stack.\n\nThe 7 layers are split into Host and Media Layers:\n\n**Host Layers:**\n\n    7- Application\n\n    6- Presentation\n\n    5- Session\n\n    4- Transport\n\n**Media Layers:**\n\n    3- Network\n\n    2- Data Link\n\n    1- Physical\n\n### Physical Layer (Layer 1)\n\n- The first and lowest layer in the OSI model\n- Responsible for the physical transmission of data over a communication channel such as wire, cable or wireless signal\n- All devices on the layer 1 network need to use the same medium for transmitting and receiving\n- This layer defines the specifications things like voltage levels, timings, data rates, distances, modulation and connectors\n- At layer 1, everything is broadcast in the medium, addresses are not used - this is addressed by layer 2\n- Collisions can happen as multiple things can broadcast at the same time. Layer 1 has no collision detection\n- For layer 1 to be useful, we need to add layer 2 which runs over the top of it\n\n### Data Link Layer (Layer 2)\n\n- Layer 2/Data Link layer is the second layer in the OSI model responsible for the reliable transfer of data between two nodes on the same network segment\n- Runs over layer 1 and requires a functional layer 1 to operate\n- MAC address is uniquely assigned to a piece of hardware\n- Layer 2 provides frames and layer 1 handles the physical transmission and reception on the medium\n- Layer 2 frames have different components:\n\nThe mac header:\n\n- **preamble** - the start of the frame - destination and source mac address - destination can be a broadcast or specific\n- Ether type specifies which layer 3 protocol is putting its data inside a frame e.g IP\n- Then the payload:\n\n**The payload**- the data the frame carries from source to destination provided by layer 3\n\n**The frame check sequence** - allows the destination to check if corruption has occurred or not\n\n- There are several devices that can be used to send data. A hub is a 'dumb' device that sits in layer 1 where as a switch sits in layer 2 and deals with addresses and collisions.\n\n### Network Layer (Layer 3)\n\nLayer 3 handles the process of moving data from a source to a destination.\n\nAny networks using only layer 2 need to use the same protocol e.g. ethernet. Because some networks don't use the same protocols, layer 3 is needed.\n\nLayer 3 can span multiple different layer 2 networks. It adds internet protocol (IP) to assign to devices to be used across different networks. Devices called routers which are layer 3 devices move packets of data across different networks.\n\nThere are two versions of IP: version 4 and 6. IP has a packet structure containing a source and destination IP field. There is a protocol field which contains a layer 4 protocol e.g. ICMP, TCP or UDP. There is also a field called TTL or time to live. This defines a maximum number of hops before a packet can be discarded so it doesn't loop forever.\n\nThe IP4 address is made up of 4 parts separated by dots with each value up to 255 e.g. 1.2.4.255. IP addresses are only dotted for human format, the actual IP address is in binary format made up of 4 x 8 bits. Each 8 bits is known as an octet. IP addresses should be unique, especially in your local network.\n\nDevices inside a local network communicate to the router. A router has a route table where each row has a destination field and a next hop/target field. Each packet that arrives at the router will have a destination.\n\n### Transport Layer (Layer 4)\n\nLayer 4, the transport layer, is responsible for ensuring that data is delivered reliably from one device to another. Layer 3 does not ensure the ordering of packet arrival nor does it deal with packet loss e.g when TTL is reached. Layer 3 also does not differentiate between the packets therefore packets from different applications are treated the same.\n\nLayer 4 remedies these issues. Layer 4 adds UDP and TCP. These run on top of IP. You would pick TCP when you want reliability and error correction. UDP is faster because it doesn't require the overhead of reliability.\n\n### Session Layer (Layer 5)\n\nTODO\n\n---\n\n### Network Address Translation (NAT)\n\nNAT is technology that allows multiple devices to share a single public IP address. They translate private IP addresses to public ones and also in reverse.\n\nThere are different types of NATs:\n\n- Static NAT which is a single 1 to 1 map of a private to fixed address\n- Dynamic NAT uses 1 private address against the 1st available public address from a pool of addresses. This is used when you have a large number of private addresses.\n- Port Address Translation - Many private addresses to 1 public address\n\nNAT only makes sense for IPv4 because IPv6 adds many more addresses.\n\n### IP Address Space and Subnetting\n\nIPv4 addresses occupy a range from 0.0.0.0 -> 255.255.255.255.255\nAlthough it covers over 4.29 billion addresses, with the current world population it will not suffice. Public IP addresses need to be allocated but private ones can be used freely.\n\nThe IPV4 public address space is split into ranges where specific subsets are assigned to specific entities:\n\n- Class A - 0.0.0.0 -> 127.255.255.255 - historically this space was For huge businesses but since it's been given up to the regional managers of the IP address space\n- Class B - 128.0.0.0 -> 191.255.255.255 - larger businesses that didn't need class A but as above, they are now addressed to IP managers\n- Class C - for those that were not big enough to fit into A or B but as with above, they're now owned by IP managers\n- Class D\n- Class E\n\nWith IPv6, the address space is 340 trillion spaces. IP addresses in IPv6 are no longer a scarce resource and don't need strict management.\n\nSubnetting is the process of breaking up IP address space into smaller pieces. CIDR allows us to break up the IP addresses with a prefix e.g. /16\n\n### Distributed Denial of Service (DDoS) attacks\n\nAn attack that is designed to overload websites.\n\nThere are different types of DDoS attacks:\n\n- Application layer - flooding HTTP\n- Protocol based attacks - SYN floods\n- Volumetric - DNS amplification\n\n### VLANS, TRUNKS and QinQ\n\n- A Virtual Area Network (VLAN) is a way to divide a single physical network into multiple logical networks.\n\n// TODO\n\n### SSL and TLS\n\nSecure Sockets Layer (SSL) and Transport Layer Security (TLS) at a high level do the same thing. TLS is newer and more secure.\n\nSSL and TLS provide privacy and data integrity between client and server.\n\nTLS:\n\n- ensures privacy via encryption\n- identity (server/client) verification\n- ensures a reliable connection by protecting against alteration of data in transit\n","n":0.029}}},{"i":31,"$":{"0":{"v":"System Design","n":0.707},"1":{"v":"\n### Domain Name System (DNS)\n\nThe phonebook of the internet. Translates domain names into their corresponding IP addresses e.g. www.mywebsite.com will direct to the an IP address such as 192.0.2.1. DNS will find the associated IP address and direct your request to the appropriate server\n\nSome examples of DNS services are Cloudflare DNS, Route 53 (AWS), Google Cloud DNS.\n\n### Load Balancer\n\nA load balancer is responsible for distributing network requests across servers to ensure optimal resource utilization, reduce latency and maintain high availability. The requirement for a load balancer depends on the scale of the architecture and the requirement for fault tolerance.  AWS Example: Elastic Load Balancer\n\n![Load balancer](/vault/assets/images/load-balancer.png)\n[source](https://codeburst.io/load-balancers-an-analogy-cc64d9430db0)\n\n### API Gateway \nAn API Gateway is a server or service which acts as a single entry point for clients to access various microservices or backend services. It can handle routing, authentication, request transformation, rate limiting and caching. It is well suited to when multiple microservices need a single entry point as it helps centralise concerns such as routing, security and monitoring. \n\nExamples of API gateways: Amazon API Gateway, Apigee, NGINX\n\n### Content Delivery Network (CDN)\n\nA distributed network of servers that store and deliver content such as images, videos, stylesheets and scripts to users from locations which are geographically closer to them. ","n":0.07}}},{"i":32,"$":{"0":{"v":"Hexagonal Architecture","n":0.707},"1":{"v":"\n### Background\n\n- Layered architecture segregates an application into different tiers - the most common is the 3 tier architecture where the application is split into the Presentation Layer, Logic Layer, Data Layer.\n\n- Layered architecture's attempts to enforce a separation of concerns. But Hexagonal Architecture was introduced on the basis that there is a risk with traditional approach in highly coupling the layers as business logic will inevitably leak from one layer to the next, particularly in the presentation layer.\n\n- Hexagonal Architecture (also known as ports and adaptors) was founded by Alistair Cockburn in 2005. Alistair argues that design issues arise when business logic is entangled with external entities. Therefore, decoupling any external actors from the application will allow it to act independently of any technology that it interfaces with it.\n\n- Hexagonal architecture uses 'ports' to allow external entities to communicate with the application. For every input and output to the application we have a port. It's a way for the application to communicate to the outside world without needing to know what it's interacting with.\n\n- External sources communicate to ports via an adaptor. The adaptor takes data from the port and converts it into the format required for the external source\n\n- The port and the application don't need to change. The adaptors are used for the outside world to transform the business domain details into necessary read/write data.\n\n- There are two sources to the application: the input and the output. Alistair describes these as the driving side and the driven side. The input is driving the application to do something where as the output is driven by the application itself.\n\n- Inputs can be apis but so can outputs which means hexagons can talk to other hexagons! This concept encapsulates domain-driven design where each hexagon is responsible for a single domain e.g. users, search, files, emails etc.\n\n### Breakdown of architecture:\n\n<br>\n<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQ5ps72uaZcEJzwnJbPhzUfEeBbN6CJ04j7hl2i3K2HHatNcsoyG2tgX2vnrN5xxDKLp5Jm5bzzmZdv/pub?w=960&amp;h=657\"> \n<br>\n\n### The Hexagon\n\n- The hexagon is the application itself. We can use 'hexagon' interchangeably with application.\n\n- The hexagon contains the business problem solutions.\n\n### Actors\n\n- Outside the hexagon are things the application interacts with. They can be humans, other applications or any hardware/software device. These are known as actors.\n\n- Actors are categorized as either drivers (or primary actors) or driven (or secondary actors).\n\n- Drivers/Primary actors trigger interactions with the application e.g. a test, a web application, a mobile app\n\n- Driven/Secondary actors are triggered by the application to provide some functionality needed by the application e.g. databases, message queues, another application\n\n- Driven actors can either be repositories where the application can also receive information from it or a recipient where the information is sent to it as a set and forget. Example of a repository is a database or storage device where as a recipient can be an SMTP server for sending an email, a printer, a text message to a phone etc.\n\n![Driver and Driven side of Hexagonal Architecture](assets/images/hexagon-in-out.png)\n\n### Ports\n\n- Ports are at the application/hexagon boundary which are interfaces that the application offers to the outside world. Ports belong to the application itself.\n\n- Driver ports are the **API** of the application. You can group the use cases into one port or spread across a few if we want to follow the Single Responsibility Principle.\n\n- If using a lot of ports, a recommended option is applying the command bus design pattern or [[CQRS pattern |architecture#command-query-responsibility-segregation-cqrs]] a port for executing commands and another port for executing queries.\n\n- A driven port is an interface for functionality to implement business logic. Driven ports are the **SPI** (service provider interface) required by the application.\n\n- The purpose of ports is to shift the design by purpose instead of technology and to have technologies be substitutable on all sides by an adapter. Ports are tech agnostic - it's up to adapters to transform data according to a specific actor's technical needs needs.\n\n![ports](assets/images/hexagon-example.png)\n\n### Adapters\n\n- Actors interact with the hexagon ports through adapters using specific technology. Adapters are **outside** the application\n\n- For any port there may be multiple adapters for each desired technology we want to use. For example, a port for notifications may have an adapter for email, text message and a phone call.\n\n- A driver adapter uses a driver port interface, converting technology requests into a technology agonistic request to a driver port.\n\n- For each driver port, there should be at least two adapters - one for the real driver and another for testing the behaviour of the port\n\n- A driven adapter implements the driven port interface, converting the technology agnostic methods of the port into specific technology methods. It essentially converts one interface to another.\n\n- We can use the [adapter design pattern](https://refactoring.guru/design-patterns/adapter/typescript/example) to implement adapters\n\n### Other\n\nBesides the above, there will be a 'composition root' or Main Component that runs at startup and builds the whole system. It should:\n\n- Initialise and configure the environment\n- For each port, it chooses the adapter implementing the port e.g. in-memory database vs postgres database\n- creates the application and injects the driven adapters as instances to the applications constructor\n- for each driver port, it chooses the driver adapter that uses the port and creates an instance of the adapter, injecting it into the applications constructor. It then runs the driver adapter instance\n\n![alt text](assets/images/task-assignment-app.png)\n\n### Example implementation references (Repositories)\n\n- https://github.com/jmgarridopaz/bluezone - Java\n- https://github.com/Sairyss/domain-driven-hexagon - Node JS\n- https://github.com/kamilmysliwiec/nest-cqrs-example - Using Nest JS CQRS\n- https://github.com/gregoryyoung/m-r - supplementary CQRS repo by Greg Young (.NET)\n\n#### References\n\n1. https://medium.com/ssense-tech/hexagonal-architecture-there-are-always-two-sides-to-every-story-bc0780ed7d9c\n2. https://scalastic.io/en/hexagonal-architecture/\n3. https://youtu.be/bDWApqAUjEI?feature=shared\n4. https://jmgarridopaz.github.io/content/hexagonalarchitecture.html#tc2-1\n5. https://web.archive.org/web/20170624023207/http://alistair.cockburn.us/Configurable+Dependency\n","n":0.034}}},{"i":33,"$":{"0":{"v":"Code Repositories","n":0.707},"1":{"v":"\n\n##### Handling multiple related projects:\n\n## Monorepos\n\nMono (single) repo (repository) is the concept of having a shared codebase of multiple projects. \n\nThe opposite of this would be to spread projects across different repositories e.g. repo-A contains BFF code, repo-B contains client code, repo-C is a micro service etc. \n\n### Advantages\n\nWhen multiple repositories exist for projects that are interlinked, you issues such as:\n- Having to create multiple pull-requests in various projects which are merged in the 'correct' order to minimize breaking changes\n- Difficulty of using updated dependencies locally before they are updated on remote e.g. npm libraries.\n\nHaving a monorepo can mean:\n- managing multiple projects is much easier\n- dependencies reflect locally immediately\n- improves collaboration between teams by ensuring ownership of ecosystem cohesiveness - poly repos pipelines are often independent and breaking changes can be missed\n\n### In practice\n\nA code repository can be structured in various ways to reflect a mono repo. An example could be:\n\n```bash\n- apps/\n--- app-one\n--- app-two\n- packages/\n--- ui\n--- utilities\n--- eslint-config\n--- ts-config\n\n```\n\n\n\n\n----\n\n#### References\n\n1. https://www.robinwieruch.de/javascript-monorepos/?ck_subscriber_id=1441143038\n\n\n\n\n-----------\n\nsome potentially interesting things:\nhttps://yarnpkg.com/features/workspaces - yarn v2 workspaces overview - shows how easy it is to tie node packages together. other relevant docs are zero installs/offline cache, both super important when it comes to monorepo on CI (re: solving 'how to manage dependencies for N projects')\nhttps://github.com/lerna/lerna - tool for managing monorepos. supports npm or yarn workspaces. yarn v2 covers a lot of its features. ive played with it a LOT and NEVER actually used it. always seemed needless/too specific to package based monorepos (read: not enterprise based monorepos)\nhttps://monorepo.guide/ - thinkmill (aus consultancy) high level intro to monorepos & links to some of their own tools / examples\nhttps://www.atlassian.com/git/tutorials/monorepos - atlassian overview of monorepo architecture & common pitfalls/strategies for solving git scaling\nhttps://www.youtube.com/watch?v=W71BTkUbdqE - old video but an overview of google's monorepo. blew my mind. their investment in tooling is insane.\nsome of my own stuff:\nhttps://github.com/meatwallace/yarks#clipboard-release-strategy - monorepo package publishing strategy outline from a tool i wrote that uses git, semantic-release and yarn v2/berry workspaces\nhttps://github.com/meatwallace/foundations/tree/master/packages - basic yarn v2 package-based monorepo using the above package for deployment)\nhttps://github.com/meatwallace/tbd - basic yarn v2 app/enterprise based monorepo. its missing anything advanced atm re: scaling but foundationally it's something i could use.\n","n":0.053}}},{"i":34,"$":{"0":{"v":"AWS","n":1}}},{"i":35,"$":{"0":{"v":"Solutions Architect Associate","n":0.577},"1":{"v":"\nNotes attributed to [this course](https://learn.cantrill.io/courses/enrolled/1820301)\n\n## AWS Accounts\n\n- An AWS account is a container for identities (users) and resources\n- Every AWS account has a root user\n- The account root user can't be restricted it has full access to everything within this account.\n- The credit card used with the root account will be the Account Payment method, everything will be billed to that card\n- AWS is a pay-as-you-go/consume platform\n- Certain resources have a free-tier\n- IAM - every AWS account comes with it's own IAM Database\n- IAM lets you create 3 different IAM profiles - Users, groups and roles\n  - Users represent humans or applications that need access to your account - this is for individual purposes\n  - Groups are a collection of related users\n  - Roles can be used for granting external access to your account\n- IAM policy - used to allow/deny access to AWS services attached to other identities\n- IAM is an identity provider, which also authenticates and authorizes\n- IAM Access Keys are long term credentials with up to 2 available per IAM user typically used in CLIs or applications\n- Access Keys are made up of Access Key ID and the Secret Access Key\n\n## Technical fundamentals\n\nPre-cursor to the concepts covered in this course summarized here:\n[[architecture.tech-fundamentals]]\n\n## AWS Fundamentals\n\n### Public vs Private services\n\n- Services can be categorized into two types: public and private services\n- AWS public and private service are separated by network access.\n- Public service is something which can be accessed using public endpoints e.g. s3\n- A private aws service runs within a vpc so only what is connected to that vpc can access it\n- AWS has three zones - the public internet zone, the private network and the AWS public zone which runs in between the public and private zone.\n- AWS public zone is where public services operate from e.g. s3\n\n### AWS Global Infrastructure\n\n- AWS have created their infrastructure platform consisting of isolated regions connected together.\n- A region is a creation of AWS which covers an area over the world which contains a full deployment of AWS infrastructure. New regions are added all the time.\n- When interacting with most AWS services you're doing it at a particular region e.g. elastic compute cloud in North virginia is different to interacting to elastic compute in Sydney.\n- AWS also provides Edge locations which are smaller than regions and they typically have only content distribution services as well as some types of edge computing. They are useful for companies like Netflix who want to store tv shows and movies as close to their customers as possible to allow low for low latency and high speed distribution\n- Some services act from a global perspective e.g. IAM\n- Regions have three main benefits:\n\n1. Each reason is separate geographically which isolates any faults\n2. Geopolitical separation - different governance depending the region\n3. Location control - tune architecture performance relative to an area\n\n- Regions also have a code e.g. Sydney is ep-southeast-2, as well as a name - Asia Pacific (Sydney)\n\nInside every region, AWS also provide multiple availability zones. These give isolated infrastructure within a region. If a region experiences an isolated issue but only one availability zone is affected, the others are likely to be still fully functional. A solutions architect may distribute the services across multiple availability zones. This is used to build resilience.\n\n### AWS Default Virtual Private Cloud (VPC)\n\n- A VPC is a Virtual Network inside AWS\n- A VPC is a regional service that operates within that region\n- A VPC by default is private and isolated. Services deployed into the same vpc can communicate but it's isolated from other vpcs and the AWS zone/public internet.\n- There are two types of VPCs per account:\n\n1. Default VPCs - can only have one per region. Configured by AWS.\n2. Custom VPCs - can have many per region. You use these in almost all serious AWS deployments to configure them how you like.\n\n- VPCs cannot communicate with each other without you configuring them to do so.\n- VPCs are regionally resilient\n- The default VPC gets a default CIDR IP range which is always the same - 172.31.0.0/16\n- A VPC can be subdivided into subnets for resilience. Each subnet inside a VPC can be put into an availability zone. The default VPC has one subnet in every availability zone in that region.\n- the default VPC assigns a public address to the services by default.\n\n### Elastic Compute Cloud (EC2)\n\n- EC2 is a service which allows you to provision virtual machines known as instances with an operating system.\n- EC2 is IAAS (Infrastructure as a Service) which provides access to virtual machines (instances)\n- An instance is just an operating system configured in a certain way\n- EC2 is a private AWS service by default - it uses VPC. You can configure it to have public access\n- An EC2 is AZ (availability zone) resilient. If the AZ fails then the instance fails\n- You can choose an instance with various sizes and capabilities\n- EC2 provides on-demand billing - per second\n- Instances can use different types of storage e.g. local host storage (ec2 host) or Elastic Block Store (EBS) which is network storage made available\n- EC2 instances have a state e.g. RUNNING -> STOPPED -> TERMINATED\n- EC2 can be moved from RUNNING TO STOPPED and back again\n- TERMINATING an instance is a one way change, you can do that from the RUNNING or STOPPED state. It's a non-reversible action\n- At a high level, an instance is composed of CPU, memory, disk and networking. You are charged for all four of those instances.\n- When an instance is STOPPED, it means no CPU, memory or network is being used therefore you won't be charged for any running costs of that instance. Storage however is still being used when it's in the stopped state which means you will be charged for it.\n- In order to have no costs for an EC2 instance you need to terminate it.\n- An Amazon Machine Image (AMI) is an image of an EC2 instance.\n- An AMI can be used to create an EC2 instance or an AMI can be created from an EC2 instance.\n- An AMI is similar to a server image in a physical server\n- An AMI contains:\n  - Attached permissions - who can use the image e.g only owner vs specific accounts vs public\n  - Root volume - the drive that boots the operating system\n  - Block device mapping - configuration which links the volumes that the AMI has and how they're presented to the operating system e.g boot vs data volume\n- EC2 can host different OS e.g. linux, windows, macos. You can connect to them via remote desktop (windows) or SSH (linux/macos)\n\n### Simple Storage Service (S3)\n\n- S3 is a global storage platform - it's regionally resilient. The data is replicated across availability zones in that regions. It can tolerate a fault in an AZ\n- S3 is a public service\n- It's used to host a large amount of data e.g. movies, audio, photos, text, large data sets\n- Economical and can be accessed via a variety of methods e.g. UI/CLI/API/HTTP\n- S3 delivers two main things:\n\n1. Objects - the data s3 stores\n2. Buckets - containers for objects\n\n**Objects** are basically files that are made up of two components:\n\n1. the object key (name) - usually the file name\n2. the object value (data) - the data or contents of the object\n\n- The bucket name needs to be **globally unique** - this is across all regions and aws accounts. It should be between 3-63 characters, all lower case, no underscores. Must start with a lowercase letter or a number. It can't be formatted like an IP address.\n\n- A bucket can hold an unlimited number of objects and an unlimited amount of data - it's an infinitely scalable service.\n\n- A bucket may show on the UI that it has folders but the underlying structure is flat and everything sits in the root. Folders are referred to as prefixes in s3 as they prefix the object names.\n\n- There is a soft limit of 100 buckets for an s3 account and a hard limit of 1000 buckets using support request.\n\n- You can have unlimited objects in a bucket, with each object able to range between 0 to 5TB in size.\n\n- S3 is not a file or block. It is an object store\n- You can't mount an s3 bucket as a drive e.g. K:\\ or /images\n- s3 is great for large scale data storage, distribution or upload\n- great for offloading - moving data from a server to the s3\n- Most services can use s3 as an INPUT or OUTPUT. s3 is a good default for data storage.\n\n### CloudFormation (CFN) basics\n\n- CloudFormation is Infrastructure as Code (IaC) which allows automation infrastructure creation, update and deletion.\n- CFN uses templates written in either YAML or JSON\n- A template:\n  - has a list of resources to do the action on (at least one - mandatory)\n  - description - the only restriction with this is if the template has an AWSTemplateFormatVersion, the description must come directly after it (this can be a trick question in the exam)\n  - metadata - controls how the UI presents the template\n  - parameters - adds fields which need to be added with input (default values could be provided)\n  - mappings - allows you to create lookup tables\n  - conditions - decision making in the template\n  - outputs - once the template is finished it can present outputs based on the resource e.g. the instance ID of the ec2\n- CloudFormation takes a template and creates a stack. A stack contains all the logical resources the template tells it to contain. CFN will create a corresponding physical resource in your AWS account.\n- You can update or delete the logical resources in the template and the template will do this to the physical resources on your account\n- CFN exists to automate infrastructure\n- CFN can be used as part of change management as it can be put in code repositories\n\n### CloudWatch (CW) Basics\n\n- CloudWatch is a support service which is used by many other AWS services. It collects and manages operational data detailing how it performs, runs or logging data\n- It performs 3 main jobs:\n\n1. **Metrics** - collects metrics from AWS products, Apps, on-premises\n2. **Logs** - collects logs as above\n3. **Events** - Cloudwatch can generate events to do something\n\n- Namespace is a container for monitoring data. It's a way of separating things into different areas e.g. AWS/EC2\n- A metric is a collection of related data points in a time ordered structure e.g. cpu utilization, network I/O or disk I/O\n- Data points are measurements of data consisting of a timestamp and value\n- Dimensions are used to separate data points within the same metric e.g. instance ID (i-xxxxx) and instance type (t3.small)\n- We can take action on metrics using alarms\n\n### Shared Responsibility Model\n\n- Shared responsibility in AWS is the principal that some areas you have to manage vs AWS have to manage\n- At a high level, AWS is responsible for the security of the cloud where as customers are responsible for the security in the cloud\n- AWS responsibilities include managing security of regions, Availability Zones and Edge locations specifically the hardware/global infrastructure.\n- AWS also manage the security around compute, storage, database and networking as well as any software that is used to provide those services\n- Customers need to take care of client side data encryption, server side encryption and network traffic protection.\n- Customers need to take care of OS, network and firewall configuration\n- Customers need to take care of platform, applications, identity and access management as well as customer data.\n\n![Shared Responsibility Model](/assets/images/srp-model.png)\n\n### High-Availability vs Fault-Tolerance vs Disaster Recovery\n\n- High Availability (HA) aims to ensure an agreed level of operational performance, usually uptime. for a higher than normal period\n- HA is about maximizing a system's online time\n- System availability is usually expressed as a percentage of uptime e.g. 99.9% a year means 8.77 hours p/year downtime\n- Fault tolerance (FA) is the property that enables a system to continue operating properly in the event of the failure of one or more of its components\n- HA is just about maximizing uptime where as FA is operating through failure e.g. a aeroplane can't just be highly available it must be fault tolerant\n- FA is much more complex and more costly to implement as you need to minimize outages but also design a system that will tolerate a failure\n- Disaster recovery (DR) is a set of policies, tools and procedures to enable the recovery or continuation of vital technology infrastructure and systems following a natural or human-induced disaster\n- You need to plan what should be done in the event of a failure. An example of DR planning is having off-site backup storage\n- DR planning should happen in advance so that the process is automated\n\n**Summary**:\n\n- HA - minimise outages\n- FA - operate through faults\n- DR - used when these don't work\n\n### Route53 (R53) Fundamentals\n\n- Route53 provides two main services:\n\n1. Allows you to register domains\n2. Hosts zones on managed nameservers it provides\n\n- Route53 is a global service with a single Database. You don't need to pick a region\n- It is globally resilient so it can tolerate the fault of multiple regions\n- Route53 provides DNS zones as well as hosting for those zones\n- Zone files are created and hosted on four managed name servers\n- Hosted zones can be public or private (VPC)\n- A hosted zone hosts DNS records (recordsets)\n\n### DNS Record Types\n\n- There are different records that can be stored in DNS:\n\n  - Nameserver (NS) - allow delegation to occur end to end in DNS. e.g example.com → ns1.example.com, ns2.example.com\n\n  - A and AAAA Records - A record will point to a v4 IP address and the AAAA will point to the v6 IP address. For example:\n\n    ** A Record:** example.com → 192.168.1.1\n\n    ** AAAA Record:** example.com → 2001:db8::1\n\n  - CNAME - Canonical Name - lets you create the equivalent of DNS shortcuts by pointing to the same A record. E.g. www.example.com → example.com\n  - MX Record - how a server can find a mail server for a specific domain. Includes a priority number\n    ```\n    example.com\n      Priority: 10 → mail1.example.com\n      Priority: 20 → mail2.example.com\n    ```\n  - TXT - allow you to add arbitrary text to a domain that must be matched to prove domain ownership.\n\n- TTL (Time To Live) is the time set by the DNS to determine how long a DNS record is cached by a resolver (DNS server o browser) before it must check for an updated record from an authoritative server.\n\n## IAM, Accounts and AWS Organisations\n\n### IAM Identity Policies\n\n- IAM policies are a type of policy which get attached to identities in AWS\n- Identities are IAM users, groups and roles\n- IAM Policies:\n\n  - provides and denies access to features in AWS\n  - Policy documents are created using JSON containing one or more statements\n  - the first part of a statement is a Sid (Statement ID) which is an optional field that lets you identify a statement and what it does. Using these is best practice to inform the reader\n  - Every statement will have a resource you're interacting with and the action you're wanting to perform on that resource\n  - The action is in the format \"service:operation\" where the operation can possibly be a wild card or a list of multiple actions\n  - Resources is the same only it matches AWS resources. Individual resources are referred to using the ARN\n  - Effect is either allow or deny. It is possible to be allowed and denied at the same time\n\n  ```json\n  <!-- Policy document example -->\n  {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n      {\n        \"Sid\": \"FullAccess\",\n        \"Effect\": \"Allow\",\n        \"Action\": [\"s3:*\"],\n        \"Resource\": [\"*\"]\n      }\n    ]\n  }\n  ```\n\n  - when there is an overlap in permissions, then both of the statements are processed where the priority begins at explicit denies. Denies overrule everything else. The second priority are explicit allows. Allows take effect unless there are explicit denies. The default if no rules are in place, the default is DENY.\n  - With the exception of the account root user, aws identity start of with NO ACCESS to aws resources.\n  - Remember: explicit DENY > explicit ALLOW > DENY\n\n- There are two types of policies: Inline policies and managed policies\n- Inline policies are when you apply individual JSON policy documents to each individual account. This is good for exceptional or special access rights for an individual as opposed to a group or a number of people\n- Managed policies are another JSON policy that you'd attach to identities in a reusable way. These should be used for the normal default rights in a business as they are low overhead\n- There are two types of managed policies: AWS managed policies and customer managed policies which you can create and manage for exact requirements\n\n### IAM Users and ARNs\n\n- IAM users are an identity used for anything required long term AWS access e.g. humans, applications or service accounts\n- A principal (a person/application) makes a request tto IAM to authenticate to a resource\n- Authentication for IAM users is done using either username and password or access keys. Access keys are usually used by applications or by humans using CLI tools.\n- Once a principal goes through the access tools, they become an authenticated identity\n- Once a principal is identified AWS knows which policies apply to an identity. This is the process of authorization.\n- Authentication is how a prinicpal can prove to IAM it's who they say they are where as AUthorization checks the policies attached to the identity to give them permission for a resource\n- ARN (Amazon Resource Name) uniquely identify resources within any AWS accounts.\n- ARN is used to allow you to refer to a single or group of resources using wild cards\n- ARNs are used in IAM policies\n- The format is:\n  `arn:partition:service:region:account-id:resource-id`\n  `arn:partition:service:region:account-id:resource-type/resource-id`\n  `arn:partition:service:region:account-id:resource-type:resource-id`\n- You can only have 5000 IAM users per account\n- An IAM User can be a member of 10 groups\n- If you have more than 5000 identifiable users then IAM users is not the right identity to use for that solution. You can fix this with IAM roles or Identity Federation.\n\n### IAM Groups\n\n- IAM groups are containers for IAM users\n- You can't log into IAM groups nor do they have credentials of their own\n- THey are used solely to manage and organise IAM users\n- an IAM user can be part of multiple IAM groups\n- Groups can have policies attached to them, both inline and managed\n- You can also have individual inline/managed policies at the user level\n- You should collect all the policies that apply to a user from their groups and individual policies and apply the same deny-allow-deny rule to work out what their permissions are\n- There is no limit for the amount of users in an IAM group but the IAM user limit of 5000 exists for the whole account\n- There is no such 'all users' group in IAM built in. You can create this and manage it manually\n- You cannot have any nesting in groups\n- There is a limit of 300 groups per account but it can be increased with a support ticket\n- Policies can be attached to resources as well for example a bucket can have a policy attached to it where it allows and denies identities access to that bucket.\n- A resource can be refer to a user or role to give permission to itself but it cannot give it to a group. This is because a group is not a true identity and they can't be referenced as a principal in a policy\n","n":0.018}}},{"i":36,"$":{"0":{"v":"CQRS","n":1},"1":{"v":"\n## Command and Query Responsibility Segregation\n\nCQRS means to segrate the responsibility between commands (write requests) and queries (read requests).\n\nCommands and queries don't need to be run on separate databases, only the responsibility and behaviours should be separate within the code. Different databases can be used to optimize reads or writes according to needs.\n\nCommands are events that took place in the domain. They are usually named with past-participle verbs e.g. OrderConfirmed\n\nCommands are requests for changes to the domain. They are usually named with a verb e.g. ConfirmOrder\n\nCommands and events are just data structures with no behaviour. They are called \"Data Transfer Objects\" (DTOs).\n\nAggregates are a larger unit of encapsulation that handles commands, applies events and have a state model encapsulated within that allows it to implement the required command validation.\n\nA command handler receives a command and brokers a result from the appropriate aggregate.\n\n#### Event sourcing\n\nStoring all the changes (events) to the system rather than just its current state.\n\nEvent sourcing is not a requirement to do CQRS but it works well with it.\n","n":0.076}}},{"i":37,"$":{"0":{"v":"Agile","n":1},"1":{"v":"\nThe ability to respond to **change**.\n\nAgile software development is iterative, focusing on small increments and evolving requirements.\n\nAgile development refers to any development process that is aligned with the concepts of the Agile [[agile.manifesto]]\n","n":0.174}}},{"i":38,"$":{"0":{"v":"Shape Up","n":0.707},"1":{"v":"\nNotes from the book (https://basecamp.com/shapeup) outlining the Shape Up methodology\n\n## Introduction\n\nShape up at a high level:\n\n- Work in **six-week cycles**\n- Shape up the work before giving it to a team. A senior group defines the key elements of the solution before it's considered ready to bet on. The solution should be concrete enough so that teams know what to do, yet abstract enough that they have room to work out the interesting details themselves\n- Give full responsibility to a small integrated team of designers and developers. Seniors spend less time managing teams as they autonomously work to define their own tasks, make adjustments to scope and work to build vertical slices.\n\n### Principles of Shaping\n\n- Shape up work at the right level of abstraction: not too vague and not too concrete.\n- Wireframes are too concrete\n- Words are too abstract - projects defined in too few words are not clear\n\n#### Steps to Shaping\n\n1. Set boundaries - figure out how much time the raw idea is worth and how to define the problem\n2. Rough out the elements - sketch a solution at a higher level of abstraction than Wireframes\n3. Address risks and rabbit holes - amend the solution, cut things out of it or specify details at certain spots\n4. Write the pitch - Once it's ready to bet on, we package it in a formal write-up called a \"pitch\". The pitch summarizes the problems, constraints, solution, rabbit holes and limitations.\n\n### Set Boundaries\n\n- Setting the appetite - this is the time budget for a standard team size. It can be either a Small batch which means its something that can be built in 1-2 weeks, or a Big Batch which takes a full six-weeks. This is assuming the team is one designer and 1-2 programmers.\n- Fixed time, variable scope - an appetite and estimate aren't the same. Estimates start with a design and end with a number. Appetites start with a number and end with a design. Fixed time, variable scope, means to decide what is core to the project and what is peripheral or unnecessary.\n- Good is relative - there is no \"best\" solution. The amount of time we set for our appetite is going to lead us to different solutions.\n- Responding to raw ideas - when we get new ideas, it's too early to commit and put into backlog. We need to give it space to learn whether it's important or what it might entail.\n- Narrow down the problem - flip from asking \"what could we build?\" to \"What's really going wrong?\". We need to narrow down the problem so that a complex request is not taken at face value.\n\nWatch out for grab bags - unclear ideas like \"redesign\" or \"refactorings\" that aren't driven by a single problem or use case.\n\n### Finding the Elements\n\n#### Move at the right speed\n\n1. Either we work alone or with a trusted partner who can keep pace with us\n2. Avoid the wrong level of detail in the drawings and sketches\n\nWork by hand using a couple of prototyping techniques: breadboarding and fat marker stretches:\n\n**Breadboarding** - a breadboard is an electrical engineering prototype that has all the components and wiring of a real device. There are three basic things we draw:\n\n1. Places: things we can navigate to like screens, dialogues or menus that pop up\n2. Affordances: things the user can act on like buttons and fields and copies.\n3. Connection lines: show how the affordances take the user from place to place\n\nWe do the above with words instead of pictures. For example, breadboarding customers turning off auto pay:\n\n![Breadboarding autopay invoice example](./assets/images/breadboarding-invoice.png)\n\n**Fat marker sketches** - A sketch made with such broad strokes that adding detail is difficult or impossible. You can do this with a larger tipped sharpie pen on paper or an iPad with a pen size at the largest diameter. Because ideas are sometimes visual, breadboarding would miss the point.\n\n![Fat marker sketch of a todo list](./assets/images/fat-marker-sketch.png)\n\n### Risks and Rabbit Holes\n","n":0.039}}},{"i":39,"$":{"0":{"v":"Manifesto","n":1},"1":{"v":"\nTODO\n","n":1}}}]}
